---
title: "BookBinders Book Club"
author: "Visha Arumugam(vcu526), Michael Grogan(ldl776),Sanyogita Apte(jlh562)"
date: "September 28, 2021"
output: html_document
---
<style type="text/css">

h1.title {
  font-size: 38px;
  text-align: center;
}
h4.author { 
  font-size: 18px;
  text-align: center;
}
h4.date { 
  font-size: 18px;
  text-align: center;
}
</style>
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readxl)
library(caret)
library(Boruta)
library(Rcpp)
library(e1071)
library(ROSE)
#setwd("~/GitHub/DA6813CaseStudies/Case2")
#setwd("~/MSDA/Fall 2021/GitHub")
#setwd("~/MSDA/Fall 2021/GitHub/DA6813CaseStudies")
setwd("~/MSDA/Fall 2021/Data Analytics Applications/Case Study 1/DA6813CaseStudies/Case2")
```


```{r readprepare}
set.seed(12345)
bbtrain<-read_excel('BBBC-Train.xlsx')
bbtest<-read_excel('BBBC-Test.xlsx')
bbtrain_lm=bbtrain
bbtest_lm =bbtest
# Check for Missing Values 
sum(is.na(bbtrain))
sum(is.na(bbtest))
bbtrain$Choice<-as.factor(bbtrain$Choice)
bbtest$Choice<-as.factor(bbtest$Choice)
#str(bbtrain)
#str(bbtest)
#remove the index column
bbtrain<-bbtrain[-1]
bbtest<-bbtest[-1]
bbtrain_lm=bbtrain_lm[-1]
bbtest_lm=bbtest_lm[-1]
#view the distribution of target variable
plot(bbtrain$Choice)

```

```{r preprocess}

#balance the target classes
bbtrain.over<-upSample(x=bbtrain[,2:ncol(bbtrain)],y=bbtrain$Choice)
bbtrain.over$Choice <- factor(bbtrain.over$Class)
bbtrain.over$Class<-NULL

bbtrain.rose=ROSE(Choice~.,data=bbtrain,seed=12345)$data
plot(bbtrain.over$Choice)
plot(bbtrain.rose$Choice)

```



```{r, echo=F}
#boruta_var_imp_output=Boruta(Choice~.,data=bbtrain,doTrace=1)
#boruta_signif <- getSelectedAttributes(boruta_var_imp_output, withTentative = TRUE)



#boruta_roug_fix_mod=TentativeRoughFix(boruta_var_imp_output)
# Variable Importance Scores#
#boruta_imps <- attStats(boruta_roug_fix_mod)
#boruta_imps2 = boruta_imps[boruta_imps$decision != 'Rejected', c('meanImp', 'decision')]
#boruta_imps2[order(-boruta_imps2$meanImp), ]

#plot(boruta_var_imp_output, cex.axis=.7, las=2, xlab="", main="Variable Importance")  

```


#Logistic Regression
```{r,models}
# Train the unbalanced dataset for logistic regression model
log<-glm(Choice~.-First_purchase,data=bbtrain,family=binomial)
summary(log)
# Train the Over-sampled for logistic regression model
log_over<-glm(Choice~.-First_purchase,data=bbtrain.over,family=binomial)
summary(log_over)
# Train the Synthetically generated  dataset for logistic regression model
log_rose<-glm(Choice~.-First_purchase,data=bbtrain.rose,family=binomial)
summary(log_rose)
```


```{r}

tunedsvm=tune("svm",Choice~.,data=bbtrain,kernel ="linear",ranges=list(cost=c( 0.001, 0.01, 1,5)))

svm<-tunedsvm$best.model

summary(svm)

tunedsvm_over=tune("svm",Choice~.,data=bbtrain.over,kernel ="linear",ranges=list(cost=c( 0.001, 0.01, 1,5)))

svm_over<-tunedsvm_over$best.model

summary(svm_over)

tunedsvm_rose=tune("svm",Choice~.,data=bbtrain.rose,kernel ="linear",ranges=list(cost=c( 0.001, 0.01, 1,5)))

svm_rose<-tunedsvm_rose$best.model
summary(svm_rose)

```
# Linear Regression
```{r}
#bbtrain$Choice<-as.numeric(as.character(bbtrain$Choice))
lm_1=lm(Choice~.,data=bbtrain_lm)
summary(lm_1)
lm<-glm(Choice~.,data=bbtrain_lm,family=gaussian)
summary(lm)
```




```{r}


predsvm<-predict(svm,bbtest)
predsvm.over<-predict(svm_over,bbtest)
predsvm.rose<-predict(svm_over,bbtest)


predlm<-predict(lm,bbtest_lm)


predlm<-as.factor(ifelse(predlm>0.5,1,0))



```

```{r,Prediction,include=F}
# Prediction using Logistic Regression with unbalanced Dataset
predlog<-predict(log,bbtest,type="response")
predlog<-as.factor(ifelse(predlog>0.5,1,0))

# Prediction using Logistic Regression with Over sampled Dataset
predlog_over<-predict(log_over,bbtest,type="response")
predlog_over<-as.factor(ifelse(predlog_over>0.5,1,0))

# Prediction using Logistic Regression with Synthetically Generated Dataset
predlog_rose<-predict(log_rose,bbtest,type="response")
predlog_rose<-as.factor(ifelse(predlog_rose>0.5,1,0))

```



```{r echo=F}

bbtest$Choice<-as.factor(bbtest$Choice)


#Prediction and test are all ordered so 0 is the positive value, so reverse them here 
bbtest$Choice<-factor(bbtest$Choice,levels=rev(levels(bbtest$Choice)))
predlog<-factor(predlog,levels=rev(levels(predlog)))
predlog_over<-factor(predlog_over,levels=rev(levels(predlog_over)))
predlog_rose<-factor(predlog_rose,levels=rev(levels(predlog_rose)))
predsvm<-factor(predsvm,levels=rev(levels(predsvm)))
predsvm.over<-factor(predsvm.over,levels=rev(levels(predsvm.over)))
predsvm.rose<-factor(predsvm.rose,levels=rev(levels(predsvm.rose)))
predlm<-factor(predlm,levels=rev(levels(predlm)))


print(caret::confusionMatrix(predlog,bbtest$Choice))
cat("\n")

print(caret::confusionMatrix(predlog_over,bbtest$Choice))
cat("\n")

print(caret::confusionMatrix(predlog_rose,bbtest$Choice))
cat("\n")

print(accuracy.meas(bbtest$Choice,predlog))

cat("\n")

print(accuracy.meas(bbtest$Choice,predlog_over))

cat("\n")

print(accuracy.meas(bbtest$Choice,predlog_rose))

cat("\n")

```
```{r}
roc.curve(bbtest$Choice,predlog,col="blue")
roc.curve(bbtest$Choice,predlog_over,add.roc = TRUE,col="green")
roc.curve(bbtest$Choice,predlog_rose,add.roc = TRUE,col="brown")
```


```{r}
print(caret::confusionMatrix(predsvm,bbtest$Choice))
cat("\n")

print(caret::confusionMatrix(predsvm.over,bbtest$Choice))
cat("\n")
print(caret::confusionMatrix(predsvm.rose,bbtest$Choice))
cat("\n")

```

```{r}
roc.curve(bbtest$Choice,predsvm,col="blue")
roc.curve(bbtest$Choice,predsvm.over,add.roc = TRUE,col="green")
roc.curve(bbtest$Choice,predsvm.rose,add.roc = TRUE,col="brown")
```

```{r}
print(caret::confusionMatrix(predlm,as.factor(bbtest_lm$Choice)))
cat("\n")
```

### I - Executive Summary





### II - The Problem

The Bookbinders Book Club is a specialty book distributor that is seeking to survive in an business environment increasingly dominated by superstores like Amazon that are able to leverage economies of scale to out-compete book clubs and smaller retail stores. In order to be more competitive, BBBC has collected data on its customers and plans to use that data to identify the characteristics of the individuals that are most likely to buy a book when mailed a specialty brochure.

The data they have on their customers is primarily numeric data relating to how many books the customer has bought from different categories such as Cooking, Art, etc.

They have specified that they want to find the most useful model out of the following options: logistic regression, linear regression, and support vector machine.

We will show how we evaluated the three models and determined the top performer, and compare the profitability of using that model to determine customers to send mailers vs sending mailers to the entire customer base.

## III - Review of Related Literature
There were very few Marketing Analysis happened with this BookBinders Book club data set and based upon the usage of various tools and technologies,various exploration and various prediction techniques, different analysis came with different conclusion and Recommendation.

Few of the Analysis examples on this data set uses the choice based analysis , which will evaluate the effectiveness of marketing efforts based on past purchase data at rather low costs.As per the choice based analysis, it is concluded that the Every person who has purchase a product within the past four months will be considered as good target in order to promote the direct marketing campaign for the book "The Art History of Florence".

Some analysis has conducted using RFM analysis (which is a marketing technique used to quantitatively rank and group customers based on the recency, frequency and monetary total of their recent transactions to identify the best customers and perform targeted marketing campaigns.) and binary logistic regression in order to promote the direct marketing campaign for the book "The Art History of Florence".

In this Analysis we are going to use Logistic Regression, Linear Regression,Support vector Machines Prediction algorithm in order to predict the customers who are potential buyers of the book "The Art History of Florence" who can be targeted for promoting the direct mail campaign.

## IV - Methodology
We are going to use three prediction algorithm technique to identify the potential buyer of the book through direct mail brochure.The three models are as follows Linear Regression,Logistic Regression and Support Vector Machine.
Prior to the training process for any of the models the variables are selected and modified for more efficient computation and accurate results. Variables that lack predictive value are removed, and numeric data describing unrelated phenomena are scaled to condense the dimensional space for the calculations.

The data then needs to be balanced for the target class, because with the unmodified data set a classifier could achieve 90% accuracy by predicting a 'no' response for every observation. A balanced training set is created by resampling the "yes" observations to match the quantity of "no" observations. However, after the classifier is trained, it will be tested on the unbalanced test set in order to determine how the classifier would perform under real conditions.

The main reason for sending the broucher through the main is to find the potential buyers to buy the book,so the model which will predict the customers will not buy the book is not useful and also it will impact the book binders club profitability by sending the broucher to the more people who will not buy the book.

The dataset contains the very few significant variables about their purchase details from the book club, so we have used the full model to identify the potential buyers inorder for book club to decide which gives higher profitability, whether to send the Book broucher through direct mail only for potential buyer or for entire club members from the database.

Also in this analysis we have find out why linear regression is not a good model to identify the potentail buyers .

Following is a brief summary of the classifiers we used:

**Linear Regression:**
  Linear regression attempts to model the relationship between dependent and independent variables by fitting a linear equation to observed data. The most common method for fitting a regression line is the method of least-squares. This method calculates the best-fitting line for the observed data by minimizing the sum of the squares of the vertical deviations from each data point to the line.
  
**Logistic Regression:**
  Logistic Regression is a parametric classification method in which is used to model the probability of a certain class or event existing based upon the independent variables.In Logistic Regression, we don’t directly fit a straight line to our data like in linear regression. Instead, we fit a S shaped curve, called Sigmoid, to our observations.
  
**Support Vector Machines:**
  SVM is a learning algorithm used in regression tasks. However, SVM  is preferable in classification tasks. This algorithm is based on the following idea: if a classifier is effective in separating convergent non-linearly separable data points, then it should perform well on dispersed ones. SVM finds the best separating line that maximizes the distance between the hyperplanes of decision boundaries.
  
### V - Data
The dataset to be used is the sample of Bookbinders Book Club customers from Pennsylvania, New York, and Ohio Which contains the details of whether the customers are willing to buy the book "The Art of Florescence" or not through direct mailing the brochure. 

Along the 1600 records in the dataset, 400 members who bought the book and 1200 who didn't bought the book, which ends up in a imbalanced dataset. As part of this Case study we have tried a Oversampling and Synthetically Data Generation sampling in order to increase the prediction of customers who will buy the book.

**Oversampling:**It replicates the observations from minority class to balance the data.An advantage of using this method is that it leads to no information loss.The disadvantage of using this method is that, since oversampling simply adds replicated observations in original data set, it ends up adding multiple observations of several types, thus leading to overfitting. 

**Synthetic Data Generation:**Instead of replicating and adding the observations from the minority class, it overcome imbalances by generates artificial data based on feature space (rather than data space) similarities from minority samples. It is also a type of oversampling technique.

```{r, echo=FALSE}
bbtrain$Choice<-as.factor(as.character(bbtrain$Choice))
plot(bbtrain$Choice,main="Unbalanced Dataset")

```
```{r,echo=FALSE}
plot(bbtrain.over$Choice,main="Over-Sampling Dataset")
```

```{r,echo=FALSE}
plot(bbtrain.rose$Choice,main="Synthetically Generated Dataset")
```
### VI - Findings


### VII - Conclusions

```{r}
#cost is .65 per mail sent, book cost is 15 with overhead of 45% of cost, and selling price is 31.95
#The following assumes that Midwest will have a similar buying population as the test data


predictions<-list(predlm,predlog,predlog_over,predlog_rose,predsvm,predsvm.over,predsvm.rose)
predlabel<-c("Balanced Linear Regression","Raw Logit","Balanced Logit","Synthetic Logit","Raw SVM","Balanced SVM","Synthetic SVM")

mailcost<-0.65
profit<-31.95-(15*1.45)
Midwestbase<-50000
buyerfraction<-sum(bbtest$Choice==1)/length(bbtest$Choice)

blanketprofit<-((Midwestbase*buyerfraction)*profit)-(Midwestbase*mailcost)

print("Profitability using Model to select mailers, vs mailing every Midwest customer")
print(paste("Profit mailing everyone: ","$",round(blanketprofit,2),sep=""))


for(i in 1:length(predictions)){

bestpredictor<-unlist(predictions[i])

#percentage of buy predictions made by model and percentage of buy predictions that are correct
detectionprevalence<-as.numeric(caret::confusionMatrix(bestpredictor,bbtest$Choice)$byClass[10])
pospredvalue<-as.numeric(caret::confusionMatrix(bestpredictor,bbtest$Choice)$byClass[3])

targetedprofit<-((Midwestbase*detectionprevalence*pospredvalue)*profit)-(Midwestbase*detectionprevalence*mailcost)

outperformance<-(targetedprofit-blanketprofit)*100/blanketprofit


print(predlabel[i])
print(paste(round(outperformance, 2), "%", sep=""))
}

```

### Appendix

#### Preprocessing the data


#### Determining most significant variables



```{r}

```

#### Training different models


#### Model output
