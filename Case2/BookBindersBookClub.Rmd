---
title: "BookBinders Book Club"
author: "Visha Arumugam(vcu526), Michael Grogan(ldl776),Sanyogita Apte(jlh562)"
date: "September 28, 2021"
output: html_document
---
<style type="text/css">

h1.title {
  font-size: 38px;
  text-align: center;
}
h4.author { 
  font-size: 18px;
  text-align: center;
}
h4.date { 
  font-size: 18px;
  text-align: center;
}
</style>
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readxl)
library(caret)
library(Boruta)
library(Rcpp)
library(InformationValue)
library(e1071)
library(ROSE)
library(pROC)
setwd("~/GitHub/DA6813CaseStudies/Case2")
#setwd("~/MSDA/Fall 2021/GitHub")
#setwd("~/MSDA/Fall 2021/GitHub/DA6813CaseStudies")
#setwd("~/MSDA/Fall 2021/Data Analytics Applications/Case Study 1/DA6813CaseStudies/Case2")
```


```{r readprepare}
set.seed(12345)
bbtrain<-read_excel('BBBC-Train.xlsx')
bbtest<-read_excel('BBBC-Test.xlsx')
bbtrain$Choice<-as.factor(bbtrain$Choice)
bbtest$Choice<-as.factor(bbtest$Choice)
#str(bbtrain)
#str(bbtest)
#remove the index column
bbtrain<-bbtrain[-1]
bbtest<-bbtest[-1]

#view the distribution of target variable
plot(bbtrain$Choice)

```


```{r preprocess}

#balance the target classes
bbtrain.over<-upSample(x=bbtrain[,2:ncol(bbtrain)],y=bbtrain$Choice)
bbtrain.over$Choice <- factor(bbtrain.over$Class)
bbtrain.over$Class<-NULL

bbtrain.rose=ROSE(Choice~.,data=bbtrain,seed=12345)$data
plot(bbtrain.over$Choice)
plot(bbtrain.rose$Choice)
```



```{r, echo=F}
#boruta_var_imp_output=Boruta(Choice~.,data=bbtrain,doTrace=1)
#boruta_signif <- getSelectedAttributes(boruta_var_imp_output, withTentative = TRUE)



#boruta_roug_fix_mod=TentativeRoughFix(boruta_var_imp_output)
# Variable Importance Scores#
#boruta_imps <- attStats(boruta_roug_fix_mod)
#boruta_imps2 = boruta_imps[boruta_imps$decision != 'Rejected', c('meanImp', 'decision')]
#boruta_imps2[order(-boruta_imps2$meanImp), ]

#plot(boruta_var_imp_output, cex.axis=.7, las=2, xlab="", main="Variable Importance")  

```


#Logistic Regression
```{r,models}
# Train the unbalanced dataset for logistic regression model
log<-glm(Choice~.-First_purchase,data=bbtrain,family=binomial)
summary(log)
# Train the Over-sampled for logistic regression model
log_over<-glm(Choice~.-First_purchase,data=bbtrain.over,family=binomial)
summary(log_over)
# Train the Synthetically generated  dataset for logistic regression model
log_rose<-glm(Choice~.-First_purchase,data=bbtrain.rose,family=binomial)
summary(log_rose)
```


```{r}

tunedsvm=tune("svm",Choice~.,data=bbtrain,kernel ="linear",ranges=list(cost=c( 0.001, 0.01, 1,5)))

svm<-tunedsvm$best.model

summary(svm)

tunedsvm_over=tune("svm",Choice~.,data=bbtrain.over,kernel ="linear",ranges=list(cost=c( 0.001, 0.01, 1,5)))

svm_over<-tunedsvm_over$best.model

summary(svm_over)

tunedsvm_rose=tune("svm",Choice~.,data=bbtrain.rose,kernel ="linear",ranges=list(cost=c( 0.001, 0.01, 1,5)))

svm_rose<-tunedsvm_rose$best.model
summary(svm_rose)

```
```{r}
bbtrain$Choice<-as.numeric(as.character(bbtrain$Choice))
lm<-glm(Choice~.,data=bbtrain,family=gaussian)

summary(lm)
```




```{r}


predsvm<-predict(svm,bbtest)
predsvm.over<-predict(svm_over,bbtest)
predsvm.rose<-predict(svm_over,bbtest)


predlm<-predict(lm,bbtest)


predlm<-as.factor(ifelse(predlm>0.5,1,0))



```

```{r,Prediction,include=F}
# Prediction using Logistic Regression with unbalanced Dataset
predlog<-predict(log,bbtest,type="response")
predlog<-as.factor(ifelse(predlog>0.5,1,0))

# Prediction using Logistic Regression with Over sampled Dataset
predlog_over<-predict(log_over,bbtest,type="response")
predlog_over<-as.factor(ifelse(predlog_over>0.5,1,0))

# Prediction using Logistic Regression with Synthetically Generated Dataset
predlog_rose<-predict(log_rose,bbtest,type="response")
predlog_rose<-as.factor(ifelse(predlog_rose>0.5,1,0))

```



```{r echo=F}

bbtest$Choice<-as.factor(bbtest$Choice)


#Prediction and test are all ordered so 0 is the positive value, so reverse them here 
bbtest$Choice<-factor(bbtest$Choice,levels=rev(levels(bbtest$Choice)))
predlog<-factor(predlog,levels=rev(levels(predlog)))
predlog_over<-factor(predlog_over,levels=rev(levels(predlog_over)))
predlog_rose<-factor(predlog_rose,levels=rev(levels(predlog_rose)))
predsvm<-factor(predsvm,levels=rev(levels(predsvm)))
predsvm.over<-factor(predsvm.over,levels=rev(levels(predsvm.over)))
predsvm.rose<-factor(predsvm.rose,levels=rev(levels(predsvm.rose)))
predlm<-factor(predlm,levels=rev(levels(predlm)))


print(caret::confusionMatrix(predlog,bbtest$Choice))
cat("\n")

print(caret::confusionMatrix(predlog_over,bbtest$Choice))
cat("\n")

print(caret::confusionMatrix(predlog_rose,bbtest$Choice))
cat("\n")

print(accuracy.meas(bbtest$Choice,predlog))

cat("\n")

print(accuracy.meas(bbtest$Choice,predlog_over))

cat("\n")

print(accuracy.meas(bbtest$Choice,predlog_rose))

cat("\n")

```
```{r}
roc.curve(bbtest$Choice,predlog,col="blue")
roc.curve(bbtest$Choice,predlog_over,add.roc = TRUE,col="green")
roc.curve(bbtest$Choice,predlog_rose,add.roc = TRUE,col="brown")
```


```{r}
print(caret::confusionMatrix(predsvm,bbtest$Choice))
cat("\n")

print(caret::confusionMatrix(predsvm.over,bbtest$Choice))
cat("\n")
print(caret::confusionMatrix(predsvm.rose,bbtest$Choice))
cat("\n")

```

```{r}
roc.curve(bbtest$Choice,predsvm,col="blue")
roc.curve(bbtest$Choice,predsvm.over,add.roc = TRUE,col="green")
roc.curve(bbtest$Choice,predsvm.rose,add.roc = TRUE,col="brown")
```

```{r}
print(caret::confusionMatrix(predlm,bbtest$Choice))
cat("\n")
```

### I - Executive Summary





### II - The Problem

The Bookbinders Book Club is a specialty book distributor that is seeking to survive in an business environment increasingly dominated by superstores like Amazon that are able to leverage economies of scale to out-compete book clubs and smaller retail stores. In order to be more competitive, BBBC has collected data on its customers and plans to use that data to identify the characteristics of the individuals that are most likely to buy a book when mailed a specialty brochure.

The data they have on their customers is primarily numeric data relating to how many books the customer has bought from different categories such as Cooking, Art, etc.

They have specified that they want to find the most useful model out of the following options: logistic regression, linear regression, and support vector machine.

We will show how we evaluated the three models and determined the top performer, and compare the profitability of using that model to determine customers to send mailers vs sending mailers to the entire customer base.


```{r}
#cost is .65 per mail sent, book cost with overhead costs 15*1.45 and selling price is 31.95
#The following assumes that Midwest will have a similar buying population as the test data

mailcost<-0.65
profit<-31.95-(15*1.45)
Midwestbase<-50000
buyerfraction<-sum(bbtest$Choice==1)/length(bbtest$Choice)

blanketprofit<-((Midwestbase*buyerfraction)*profit)-(Midwestbase*mailcost)

#bestpredictor<-factor(predsvm,levels=rev(levels(predsvm)))

bestpredictor<-predsvm.over

#percentage of buy predictions made by model and percentage of buy predictions that are correct
detectionprevalence<-as.numeric(caret::confusionMatrix(bestpredictor,bbtest$Choice)$byClass[10])
pospredvalue<-as.numeric(caret::confusionMatrix(bestpredictor,bbtest$Choice)$byClass[3])

targetedprofit<-((Midwestbase*detectionprevalence*pospredvalue)*profit)-(Midwestbase*detectionprevalence*mailcost)

outperformance<-(targetedprofit-blanketprofit)*100/blanketprofit


print(blanketprofit)
print(targetedprofit)
paste(round(outperformance, 2), "%", sep="")


```



## III - Review of Related Literature

## IV - Methodology


### V - Data
The dataset to be used is the sample of Bookbinders Book Club customers from Pennsylvania, New York, and Ohio  Which contains the details of whether the customers are willing to buy the book "The Art of Florescence" or not through direct mailing the brocher. 

Along the 1600 records in the dataset, 400 members who bought the book and 1200 who didn't bought the book, which ends up in a imbalanced dataset. As part of this Case study we have tried a Oversampling and Synthetically Data Generation sampling in order to increase the prediction of customers who will buy the book.

**Oversampling:**It replicates the observations from minority class to balance the data.An advantage of using this method is that it leads to no information loss.The disadvantage of using this method is that, since oversampling simply adds replicated observations in original data set, it ends up adding multiple observations of several types, thus leading to overfitting. 

**Synthetic Data Generation:**Instead of replicating and adding the observations from the minority class, it overcome imbalances by generates artificial data based on feature space (rather than data space) similarities from minority samples. It is also a type of oversampling technique.

```{r, echo=FALSE}
plot(bbtrain$Choice,main="Unbalanced Dataset")

```
```{r,echo=FALSE}
plot(bbtrain.over$Choice,main="Over-Sampling Dataset")
```

```{r,echo=FALSE}
plot(bbtrain.rose$Choice,main="Synthetically Generated Dataset")
```
### VI - Findings


### VII - Conclusions


### Appendix

#### Preprocessing the data


#### Determining most significant variables




#### Training different models


#### Model output
