---
title: "BankMarketing"
group members: "Michael Grogan"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(rpart.plot)
library(MASS)
setwd("~/GitHub/DA6813CaseStudies")
```

## I - Executive Summary
## II - The Problem

The task is to analyze the dataset of customers provided by the banking institution, and determine what combination of personal characteristics and external factors are associated with the highest likelihood of the client subscribing to a term deposit. 

Ascertaining which factors are most correlated with subscribing to term deposits will allow the bank to save time and money by focusing on clients that are likely to subscribe, and avoiding those that are not.

## III - Review of Related Literature
There were few Marketing Analysis happened with this bank marketing data set and based upon the usage of various exploration, prediction techniques and hyper parameter optimization, different analysis came with different conclusion and Recommendation.
As per the Journal of Visualization and Analysis in bank Direct marketing prediction by Alaa abu-srhan, based on the exploratory data analysis , different oversampling methods such as Synthetic Minority Oversampling technique, Random Over Oversampling Technique, Selective Pre-processing, etc are used to overcome the imbalance in the response variable which in turn increase prediction accuracy from various classification prediction techniques. Following are the classification techniques,(Random forest, support
vector machine (SVM), neural network (NN), Naive Bayes,and k-nearest neighbor (KNN) classifiers), are used for the analysis and the results are compared on Gmean and accuracy evaluation metrics to identify the best results.As per the conclusion, SVM and Naive Bayes Classifier provides a better accuracy and Gmean values.
  
  Similarly in Data Mining- Bank Marketing Data set by "Kinga WÅ‚odarczyk", Different classifier Techniques such as KNN, Linear and logistic regression models have created using different dependent variables in order to predict the response by comparing the accuracy.
  
  In this analysis we are also going to try different classification techniques such as logistic regression, random forest, support vector machines with different hyper parameters and different predictors and compare the results based on the accuracy in order to identify the appropriate model.  

## IV - Methodology

Read data and check for missing values
As is outlined in the case, "duration" serves no predictive purpose, because it can only be known after the call is made, and can't be used in the decision for which customer to call, so it is removed. Also removed is the pdays variable which lacks sufficient variance to provide meaningful predictive value.

Finally numeric data is normally scaled in order to condense the dimension space for calculating support vectors.

The data also needs to be balanced, because with the unmodified dataset, a classifier could achieve 90% accuracy by predicting a 'no' response for every observation.

Upsampling changes y variable name to Class.

First get an idea of most important variables using stepwise feature selection using AIC

The full model doesn't yield more accurate predictions than the step-selected model, so the different models are tested with the limited set of significant predictors.

Test several models (logistic regression, decision tree, random forest, linear SVC) using bootstrapped cross validation to optimize tuning variables



## V - Data

```{r readprepare}
bank<-read.csv('bank-additional.csv',sep=";",stringsAsFactors = T)
sum(is.na(bank))
#duration in column 11
bank<-bank[-11]

#replace month abbreviations with numbers
monthord<-as.character(bank$month)
mn<-c('jan','feb','mar','apr','may','jun','jul',
  'aug','sep','oct','nov','dec')
md<-c(1,2,3,4,5,6,7,8,9,10,11,12)
monthord[monthord %in% mn] <- md[match(monthord, mn)]
bank$month<-as.factor(monthord)

#replace day of week abbreviations with numbers
weekord<-as.character(bank$day_of_week)
mn<-c('mon','tue','wed','thu','fri','sat','sun')
md<-c(1,2,3,4,5,6,7)
weekord[weekord %in% mn] <- md[match(weekord, mn)]
bank$day_of_week<-as.factor(weekord)

#reverse levels so "yes" is returned as the positive class to the caret model
bank$y <- factor(bank$y, levels=rev(levels(bank$y)))

#scale and remove variables with near-zero variance(which turns out to be pdays)
params<-preProcess(bank,method=c("scale","nzv"))
scaledbank<-predict(params,bank)

head(scaledbank)


```
```{r trainsplit}
train<-sample(nrow(scaledbank),0.7*nrow(scaledbank))
banktrain<-scaledbank[train,]
banktest<-scaledbank[-train,]

banktrain<-upSample(x=banktrain[,-ncol(banktrain)],y=banktrain$y)


```


```{r steplog}
fullmodel<-glm(Class~.,banktrain,family=binomial)
stepped<-stepAIC(fullmodel, direction = "both",trace = FALSE)
summary(stepped)
```



```{r plots}
attach(banktrain)
plot(Class,contact,ylab="Contact")
plot(Class,age,ylab="Age")
plot(Class,marital,ylab="Marital Status")
plot(job,Class,xlab="Job")
plot(Class,campaign,ylab="Campaign")
plot(month,Class,xlab="Month")
plot(poutcome,Class,xlab="Outcome")
plot(Class,cons.price.idx,ylab="Price Index")
plot(Class,cons.conf.idx,ylab="Consumer Confidence")
plot(Class,emp.var.rate,ylab="Employment")
detach()
```

```{r logreg}

set.seed(12345)


TC <- trainControl(method = "boot", number=50,classProbs=TRUE,summaryFunction = twoClassSummary)
#TC <- trainControl(method="repeatedcv", number=10, repeats=3)

bankLOG <- train(Class~previous+contact+campaign+cons.price.idx+cons.conf.idx+emp.var.rate+education+day_of_week, data=banktrain,
                 method = "glmnet",
                 metric="Spec",
                 trControl = TC)
bankLOGfull <- train(Class~., data=banktrain,
                 method = "glmnet",
                 metric="Spec",
                 trControl = TC)




predlog<-predict(bankLOG,banktest)

predlogfull<-predict(bankLOGfull,banktest)

print("Selected Model")
confusionMatrix(predlog,banktest$y)

print("Full Model")
confusionMatrix(predlogfull,banktest$y)


```

```{r dtree}

TC <- trainControl(method = "boot", number=50,classProbs=TRUE,summaryFunction = twoClassSummary)
#TC <- trainControl(method="repeatedcv", number=10, repeats=3)

banktree=train(Class~.,data=banktrain, metric="ROC",trControl=TC,method='rpart',control = rpart.control(cp=0.05,minsplit = 15, minbucket = 6))

rpart.plot(banktree$finalModel)


#banktree<-rpart(Class~., data=banktrain, method="class", control=rpart.control(minsplit=15, cp=0.01))
#library(rattle)
#fancyRpartPlot(banktree)

predtree<-predict(banktree,banktest)

confusionMatrix(predtree,banktest$y)

```




```{r linearSVM}
set.seed(1)


#TC <- trainControl(method = "boot", number=50,classProbs=TRUE,summaryFunction = twoClassSummary)
TC <- trainControl(method="repeatedcv", number=10,repeats=3,
                   classProbs=TRUE,summaryFunction = twoClassSummary)

bankrad <- train(Class~poutcome+job+contact+campaign+month+cons.price.idx+cons.conf.idx+emp.var.rate, data=banktrain,
                 method = "svmLinear",
                 metric="ROC",
                 trControl = TC,
                 tuneGrid = expand.grid(C = seq(0.5, 3, length = 10)))
#)
bankrad

predrad<-predict(bankrad,banktest)


confusionMatrix(predrad,banktest$y)
```



```{r randomforest}

set.seed(12345)


TC <- trainControl(method = "boot", number=50,classProbs=TRUE,summaryFunction = twoClassSummary)


bankrf <- train(Class~poutcome+job+loan+contact+campaign+month+cons.price.idx+cons.conf.idx+emp.var.rate+education+day_of_week, data=banktrain,
                 method = "rf",
                 metric="ROC",
                 trControl = TC)
bankrf

predrf<-predict(bankrf,banktest)


confusionMatrix(predrf,banktest$y)
```



## VI - Findings

Logistic regression identifies a slightly higher percentage of "yes" (subscribing to term deposits) clients, but has a lower overall accuracy than the linear SVM. This is because the rate of false "yes" predictions is twice as high for the logistic regression model. The random forest is less accurate than the SVM and has fewer true positives than the logistic regression or SVM. The method

The importance of the false positives in choosing the model depends on the cost of each call compared with the average revenue from a successful call. If that ratio is very large, it will be worth it to sacrifice potential successes in order to avoid wasted calls. 
If each call is very expensive, the decision tree method has the highest accuracy in detecting wasted calls, but it misses more "yes" calls than it accurately predicts.

## VII - Conclusions




```{r}

```

