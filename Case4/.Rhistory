geom_smooth(method = "gam", formula = y ~ s(x, bs = "cs"), se = FALSE, color = "black")+
labs(x = i, y = "Predicted acquisition")
)
}
for(i in acq_vars[-2]){
sequence_<-seq(quantile(AR[[i]])[[2]],quantile(AR[[i]])[[4]],(quantile(AR[[i]])[[2]]-quantile(AR[[i]])[[4]])/-30)
means.exp<-partial(untuned_acq,partial.xvar = i,
partial.values = sequence_)$regrOutput$acquisition %>% colMeans()
marginal.effect.df <-
data.frame(pred.acquisition = means.exp, sequence_ = sequence_)
print(
ggplot(marginal.effect.df, aes(x = sequence_, y = pred.acquisition)) +
geom_point(shape = 20, color = "purple", size = 2, stroke = 1.5)+
geom_smooth(method = "gam", formula = y ~ s(x, bs = "cs"), se = FALSE, color = "black")+
labs(x = i, y = "Predicted acquisition")
)
}
for(i in acq_vars){
sequence_<-seq(quantile(AR[[i]])[[2]],quantile(AR[[i]])[[4]],(quantile(AR[[i]])[[2]]-quantile(AR[[i]])[[4]])/-30)
means.exp<-partial(untuned_acq,partial.xvar = i,
partial.values = sequence_)$regrOutput$acquisition %>% colMeans()
marginal.effect.df <-
data.frame(pred.acquisition = means.exp, sequence_ = sequence_)
print(
ggplot(marginal.effect.df, aes(x = sequence_, y = pred.acquisition)) +
geom_point(shape = 20, color = "purple", size = 2, stroke = 1.5)+
geom_smooth(method = "gam", formula = y ~ s(x, bs = "cs"), se = FALSE, color = "black")+
labs(x = i, y = "Predicted acquisition")
)
}
length(dur_vars)
for(i in dur_vars[-7]){
sequence_<-seq(quantile(AR[[i]])[[2]],quantile(AR[[i]])[[4]],(quantile(AR[[i]])[[2]]-quantile(AR[[i]])[[4]])/-30)
means.exp<-partial(untuned_dur,partial.xvar = i,
partial.values = sequence_)$regrOutput$duration %>% colMeans()
marginal.effect.df <-
data.frame(pred.duration = means.exp, sequence_ = sequence_)
print(
ggplot(marginal.effect.df, aes(x = sequence_, y = pred.duration)) +
geom_point(shape = 20, color = "purple", size = 2, stroke = 1.5)+
geom_smooth(method = "gam", formula = y ~ s(x, bs = "cs"), se = FALSE, color = "black")+
labs(x = i, y = "Predicted duration")
)
}
knitr::opts_chunk$set(echo = TRUE)
library(SMCRM) # CRM data
library(dplyr) # data wrangling
library(tidyr) # data wrangling
library(ggplot2) # plotting
library(survival) # survival
library(rpart) # DT
library(randomForestSRC) # RF
library(sampleSelection)
#library(micEcon)
data(acquisitionRetention)
AR<-acquisitionRetention
idx.train <- sample(1:nrow(AR), size = 0.7 * nrow(AR))
train <- AR[idx.train,]
test <- AR[-idx.train,]
set.seed(123)
acq_vars<-c("acq_exp","acq_exp_sq","industry","revenue","employees")
acq_target<-"acquisition"
acq_formula<-as.formula(paste(acq_target,paste(acq_vars,collapse="+"),sep="~"))
dur_vars<-c("ret_exp","ret_exp_sq","freq","freq_sq","crossbuy","sow","IMR")
dur_target<-"duration"
dur_formula<-as.formula(paste(dur_target,paste(dur_vars,collapse="+"),sep="~"))
###Logistic Acquisition prediction
acq_lm<-glm(acq_formula,train,family=binomial(link="probit"))
acq_lm_train_pred<-ifelse(predict(acq_lm,train,type="response")>0.5,1,0)
#Confusion Matrix
#table(acq_lm_train_pred,train$acquisition)
acq_lm_train_acc<-sum(acq_lm_train_pred==train$acquisition)/length(train$acquisition)
acq_lm_test_pred<-ifelse(predict(acq_lm,test,type="response")>0.5,1,0)
#Confusion Matrix
#table(acq_lm_test_pred,test$acquisition)
acq_lm_test_acc<-sum(acq_lm_test_pred==test$acquisition)/length(test$acquisition)
####Logistic Duration Prediction
train$IMR<-invMillsRatio(glm(acq_formula,train,family=binomial(link="probit")))$IMR1
dur_lm<-glm(dur_formula,train,family=gaussian)
dur_lm_train_RMSE<-sqrt(mean(dur_lm$residuals^2))
test$IMR<-invMillsRatio(glm(acq_formula,test,family=binomial(link="probit")))$IMR1
dur_pred<-predict(dur_lm,test)
dur_lm_test_RMSE<-sqrt(mean((dur_pred-test$duration)^2))
acq_dt<-rpart(acq_formula,train)
dur_dt<-rpart(dur_formula,train)
acq_dt_train_pred<-ifelse(predict(acq_dt,train)>0.5,1,0)
acq_dt_train_acc<-sum(acq_dt_train_pred==train$acquisition)/length(train$acquisition)
#Confusion Matrix
#table(acq_dt_train_pred,train$acquisition)
acq_dt_test_pred<-ifelse(predict(acq_dt,test)>0.5,1,0)
acq_dt_test_acc<-sum(acq_dt_test_pred==test$acquisition)/length(test$acquisition)
#Confusion Matrix
#table(acq_dt_test_pred,test$acquisition)
dur_dt_train_pred<-predict(dur_dt,train)
dur_dt_train_RMSE<-sqrt(mean((dur_dt_train_pred-train$duration)^2))
dur_dt_test_pred<-predict(dur_dt,test)
dur_dt_test_RMSE<-sqrt(mean((dur_dt_test_pred-test$duration)^2))
untuned_acq <- rfsrc(acq_formula,data = train,importance = TRUE)
acq_untunedrf_train_pred<-ifelse(predict(untuned_acq,train)$predicted>0.5,1,0)
acq_untunedrf_train_acc<-sum(acq_untunedrf_train_pred==train$acquisition)/length(train$acquisition)
#Confusion Matrix
#table(acq_untunedrf_train_pred,train$acquisition)
acq_untunedrf_test_pred<-ifelse(predict(untuned_acq,test)$predicted>0.5,1,0)
acq_untunedrf_test_acc<-sum(acq_untunedrf_test_pred==test$acquisition)/length(test$acquisition)
#Confusion Matrix
#table(acq_untunedrf_test_pred,test$acquisition)
untuned_dur <- rfsrc(dur_formula,data = train,importance = TRUE)
dur_untunedrf_train_pred<-predict(untuned_dur,train)$predicted
dur_untunedrf_train_RMSE<-sqrt(mean((dur_untunedrf_train_pred-train$duration)^2))
dur_untunedrf_test_pred<-predict(untuned_dur,test)$predicted
dur_untunedrf_test_RMSE<-sqrt(mean((dur_untunedrf_test_pred-test$duration)^2))
# Establish a list of possible values for hyper-parameters
mtry.values <- seq(3,8,1)
nodesize.values <- seq(4,12,2)
ntree.values <- seq(3e3,9e3,1e3)
# Create a data frame containing all combinations
hyper_grid <- expand.grid(mtry = mtry.values, nodesize = nodesize.values, ntree = ntree.values)
# Create an empty vector to store OOB error values
oob_err <- c()
# Write a loop over the rows of hyper_grid to train the grid of models
for (i in 1:nrow(hyper_grid)) {
# Train a Random Forest model
model <- rfsrc(acq_formula,data = train,
mtry = hyper_grid$mtry[i],
nodesize = hyper_grid$nodesize[i],
ntree = hyper_grid$ntree[i])
# Store OOB error for the model
oob_err[i] <- model$err.rate[length(model$err.rate)]
}
# Identify optimal set of hyperparmeters based on OOB error
opt_i <- which.min(oob_err)
tuned_acq <- rfsrc(acq_formula,data = train,
mtry = hyper_grid[opt_i,1],
nodesize = hyper_grid[opt_i,2],
ntree = hyper_grid[opt_i,3])
acq_tunedrf_train_pred<-ifelse(predict(tuned_acq,train)$predicted>0.5,1,0)
acq_tunedrf_train_acc<-sum(acq_tunedrf_train_pred==train$acquisition)/length(train$acquisition)
#Confusion Matrix
#table(acq_tunedrf_train_pred,train$acquisition)
acq_tunedrf_test_pred<-ifelse(predict(tuned_acq,test)$predicted>0.5,1,0)
acq_tunedrf_test_acc<-sum(acq_tunedrf_test_pred==test$acquisition)/length(test$acquisition)
#Confusion Matrix
#table(acq_tunedrf_test_pred,test$acquisition)
# Establish a list of possible values for hyper-parameters
mtry.values <- seq(3,8,1)
nodesize.values <- seq(4,12,2)
ntree.values <- seq(3e3,9e3,1e3)
# Create a data frame containing all combinations
hyper_grid <- expand.grid(mtry = mtry.values, nodesize = nodesize.values, ntree = ntree.values)
# Create an empty vector to store OOB error values
oob_err <- c()
# Write a loop over the rows of hyper_grid to train the grid of models
for (i in 1:nrow(hyper_grid)) {
# Train a Random Forest model
model <- rfsrc(dur_formula,data = train,
mtry = hyper_grid$mtry[i],
nodesize = hyper_grid$nodesize[i],
ntree = hyper_grid$ntree[i])
# Store OOB error for the model
oob_err[i] <- model$err.rate[length(model$err.rate)]
}
# Identify optimal set of hyperparmeters based on OOB error
opt_i <- which.min(oob_err)
tuned_dur <- rfsrc(dur_formula,data = train,
mtry = hyper_grid[opt_i,1],
nodesize = hyper_grid[opt_i,2],
ntree = hyper_grid[opt_i,3])
dur_tunedrf_train_pred<-predict(tuned_dur,train)$predicted
dur_tunedrf_train_RMSE<-sqrt(mean((dur_tunedrf_train_pred-train$duration)^2))
dur_tunedrf_test_pred<-predict(tuned_dur,test)$predicted
dur_tunedrf_test_RMSE<-sqrt(mean((dur_tunedrf_test_pred-test$duration)^2))
testaccuracies<-c(acq_tunedrf_test_acc,acq_untunedrf_test_acc,acq_lm_test_acc,acq_dt_test_acc)
trainaccuracies<-c(acq_tunedrf_train_acc,acq_untunedrf_train_acc,acq_lm_train_acc,acq_dt_train_acc)
testRMSE<-c(dur_tunedrf_test_RMSE,dur_untunedrf_test_RMSE,dur_lm_test_RMSE,dur_dt_test_RMSE)
trainRMSE<-c(dur_tunedrf_train_RMSE,dur_untunedrf_train_RMSE,dur_lm_train_RMSE,dur_dt_train_RMSE)
model<-c("Tuned Random Forest","Untuned Random Forest","Linear Model","Decision Tree")
results<-data.frame(model,trainaccuracies,trainRMSE,testaccuracies,testRMSE)
results
for(i in acq_vars){
sequence_<-seq(quantile(AR[[i]])[[2]],quantile(AR[[i]])[[4]],(quantile(AR[[i]])[[2]]-quantile(AR[[i]])[[4]])/-30)
means.exp<-partial(untuned_acq,partial.xvar = i,
partial.values = sequence_)$regrOutput$acquisition %>% colMeans()
marginal.effect.df <-
data.frame(pred.acquisition = means.exp, sequence_ = sequence_)
print(
ggplot(marginal.effect.df, aes(x = sequence_, y = pred.acquisition)) +
geom_point(shape = 20, color = "purple", size = 2, stroke = 1.5)+
geom_smooth(method = "gam", formula = y ~ s(x, bs = "cs"), se = FALSE, color = "black")+
labs(x = i, y = "Predicted acquisition")
)
}
for(i in dur_vars[-7]){
sequence_<-seq(quantile(AR[[i]])[[2]],quantile(AR[[i]])[[4]],(quantile(AR[[i]])[[2]]-quantile(AR[[i]])[[4]])/-30)
means.exp<-partial(untuned_dur,partial.xvar = i,
partial.values = sequence_)$regrOutput$duration %>% colMeans()
marginal.effect.df <-
data.frame(pred.duration = means.exp, sequence_ = sequence_)
print(
ggplot(marginal.effect.df, aes(x = sequence_, y = pred.duration)) +
geom_point(shape = 20, color = "purple", size = 2, stroke = 1.5)+
geom_smooth(method = "gam", formula = y ~ s(x, bs = "cs"), se = FALSE, color = "black")+
labs(x = i, y = "Predicted duration")
)
}
data.frame(importance = untuned_acq$importance) %>%
tibble::rownames_to_column(var = "variable") %>%
ggplot(aes(x = reorder(variable,importance), y = importance)) +
geom_bar(stat = "identity", fill = "violet", color = "black")+
coord_flip() +
labs(x = "Variables", y = "Variable importance")
data.frame(importance = untuned_dur$importance) %>%
tibble::rownames_to_column(var = "variable") %>%
ggplot(aes(x = reorder(variable,importance), y = importance)) +
geom_bar(stat = "identity", fill = "violet", color = "black")+
coord_flip() +
labs(x = "Variables", y = "Variable importance")
rattle::fancyRpartPlot(dur_dt, sub = "")
rattle::fancyRpartPlot(acq_dt, sub = "")
knitr::opts_chunk$set(echo = TRUE)
library(SMCRM) # CRM data
library(dplyr) # data wrangling
library(tidyr) # data wrangling
library(ggplot2) # plotting
library(survival) # survival
library(rpart) # DT
library(randomForestSRC) # RF
data(acquisitionRetention)
AR<-acquisitionRetention
set.seed(123)
View(customerChurn)
dt.duration <- rpart(duration~.,data = AR)
dt.acquisition <- rpart(acquisition~acq_exp+acq_exp_sq+industry+revenue+employees,data = AR)
rattle::fancyRpartPlot(dt.duration, sub = "")
rattle::fancyRpartPlot(dt.acquisition, sub = "")
forest_acq <- rfsrc(acquisition~acq_exp+acq_exp_sq+
industry+revenue+employees,data = AR,
importance = TRUE,
ntree = 1000)
forest_dur <- rfsrc(duration~.,data = AR,
importance = TRUE,
ntree = 1000)
forest_acq
forest_dur
data.frame(importance = forest_acq$importance) %>%
tibble::rownames_to_column(var = "variable") %>%
ggplot(aes(x = reorder(variable,importance), y = importance)) +
geom_bar(stat = "identity", fill = "violet", color = "black")+
coord_flip() +
labs(x = "Variables", y = "Variable importance")
mindepth <- max.subtree(forest_acq,
sub.order = TRUE)
# first order depths
print(round(mindepth$order, 3)[,1])
# vizualise MD
data.frame(md = round(mindepth$order, 3)[,1]) %>%
tibble::rownames_to_column(var = "variable") %>%
ggplot(aes(x = reorder(variable,desc(md)), y = md)) +
geom_bar(stat = "identity", fill = "orange", color = "black", width = 0.2)+
coord_flip() +
labs(x = "Variables", y = "Minimal Depth")
# interactions
mindepth$sub.order
as.matrix(mindepth$sub.order) %>%
reshape2::melt() %>%
data.frame() %>%
ggplot(aes(x = Var1, y = Var2, fill = value)) +
scale_x_discrete(position = "top") +
geom_tile(color = "white") +
viridis::scale_fill_viridis("Relative min. depth") +
labs(x = "", y = "")
# cross-check with vimp
find.interaction(forest_acq,
method = "vimp",
importance = "permute")
summary(AR$acq_exp)
# extract marginal effect using partial dependence
acq_exp_seq = seq(350,645,5)
marginal.effect <- partial(forest_acq,
partial.xvar = "acq_exp",
partial.values = acq_exp_seq)
means.exp <- marginal.effect$regrOutput$acquisition %>% colMeans()
marginal.effect.df <-
data.frame(pred.acquisition = means.exp, acq_exp_seq = acq_exp_seq)
ggplot(marginal.effect.df, aes(x = acq_exp_seq, y = pred.acquisition)) +
geom_point(shape = 21, color = "purple", size = 2, stroke = 1.2)+
geom_smooth(method = "lm", formula = y ~ poly(x,3), se = FALSE, color = "black")+ # try with other values
labs(x = "Average acquisition in $", y = "Predicted acquisition") +
scale_x_continuous(breaks = seq(0,150,25))
table(AR$crossbuy)
forest_acq$xvar$acq_exp[forest_acq$xvar$crossbuy == 2]
forest_acq$xvar
seq(15,65,10)
grp <- seq(15,65,10)
get_coplot_data <- function(i) {
subset.coplot <- forest_acq$xvar$acq_exp[forest_acq$xvar$employees > i]
coplot <- plot.variable(forest_acq,
xvar.names = "acq_exp",
partial = TRUE,
subset = subset.coplot) }
coplot_data_list <- purrr::map(grp,get_coplot_data)
coplot.df <- data.frame(acq_exp =
c(coplot_data_list[[1]]$pData[[1]]$x.uniq,
coplot_data_list[[2]]$pData[[1]]$x.uniq,
coplot_data_list[[3]]$pData[[1]]$x.uniq,
coplot_data_list[[4]]$pData[[1]]$x.uniq,
coplot_data_list[[5]]$pData[[1]]$x.uniq,
coplot_data_list[[6]]$pData[[1]]$x.uniq),
predicted_acquisition =
c(coplot_data_list[[1]]$pData[[1]]$yhat,
coplot_data_list[[2]]$pData[[1]]$yhat,
coplot_data_list[[3]]$pData[[1]]$yhat,
coplot_data_list[[4]]$pData[[1]]$yhat,
coplot_data_list[[5]]$pData[[1]]$yhat,
coplot_data_list[[6]]$pData[[1]]$yhat),
groups = as.factor(rep(grp, each = 25)))
coplot.df %>%
filter(groups %in% c(45,55)) %>%
ggplot(aes(x = acq_exp, y = predicted_acquisition, fill = groups, color = groups))+
geom_point(shape = 21, color = "black", size = 4, stroke = 1.2)+
stat_smooth() +
scale_fill_brewer(palette = "Set1") +
scale_color_brewer(palette = "Set1") +
labs(y = "Predicted acquisition", x = "Average acquisition expense in $") +
guides(fill = guide_legend(override.aes = list(linetype = 0)))
proximity_mat <- rfsrc(acquisition~acq_exp+acq_exp_sq+
industry+revenue+employees,
data = AR,
importance = TRUE,
ntree = 1000,
proximity = "oob")$proximity
proximity_mat[1:50,1:50] %>%
as.data.frame(col.names = 1:nrow(AR)) %>%
tibble::rowid_to_column() %>%
gather(key = columnid, value = proximity, -rowid) %>%
mutate(columnid = as.numeric(substr(columnid,2,3))) %>%
ggplot(aes(x = rowid, y = columnid, fill = proximity))+
geom_tile() +
viridis::scale_fill_viridis("Proximity" ,option = "D") +
labs(x = "Case #", y = "Case #")
set.seed(123)
idx.train <- sample(1:nrow(AR), size = 0.7 * nrow(AR))
train.df <- AR[idx.train,]
test.df <- AR[-idx.train,]
forest_acqT <- rfsrc(acquisition~acq_exp+acq_exp_sq+
industry+revenue+employees,data = train.df,
importance = TRUE,
ntree = 1000)
forest_acqT$err.rate[length(forest_acqT$err.rate)]
data.frame(forest_acqT = forest_acqT$err.rate) %>%
na.omit() %>%
tibble::rownames_to_column(var = "trees") %>%
mutate(trees = as.numeric(trees)) %>%
gather(key = forest_type, value = OOB.err, -trees) %>%
ggplot(aes(x = trees, y = OOB.err, color = forest_type))+
geom_line()+
scale_color_brewer(palette = "Set1")+
scale_x_continuous(breaks = seq(0,1050,100))+
labs(x = "Number of trees", y = "OOB Error rate")
# Establish a list of possible values for hyper-parameters
mtry.values <- seq(4,6,1)
nodesize.values <- seq(4,8,2)
ntree.values <- seq(4e3,6e3,1e3)
# Create a data frame containing all combinations
hyper_grid <- expand.grid(mtry = mtry.values, nodesize = nodesize.values, ntree = ntree.values)
# Create an empty vector to store OOB error values
oob_err <- c()
# Write a loop over the rows of hyper_grid to train the grid of models
for (i in 1:nrow(hyper_grid)) {
# Train a Random Forest model
model <- rfsrc(acquisition~acq_exp+acq_exp_sq+
industry+revenue+employees,
data = train.df,
mtry = hyper_grid$mtry[i],
nodesize = hyper_grid$nodesize[i],
ntree = hyper_grid$ntree[i])
# Store OOB error for the model
oob_err[i] <- model$err.rate[length(model$err.rate)]
}
# Identify optimal set of hyperparmeters based on OOB error
opt_i <- which.min(oob_err)
print(hyper_grid[opt_i,])
tuned <- rfsrc(acquisition~acq_exp+acq_exp_sq+
industry+revenue+employees,
data = train.df,
mtry = 4,
nodesize = 8,
ntree = 6000)
error.df <-
data.frame(pred1 = predict.rfsrc(forest_acqT,newdata = test.df)$predicted,
pred2 = predict.rfsrc(tuned, newdata = test.df)$predicted,
actual = test.df$acquisition,
customer = test.df$customer) %>%
mutate_at(.funs = funs(abs.error = abs(actual - .),
abs.percent.error = abs(actual - .)/abs(actual)),
.vars = vars(pred1:pred2))
#mae
error.df %>%
summarise_at(.funs = funs(mae = mean(.)),
.vars = vars(pred1_abs.error:pred2_abs.error))
#mape
error.df %>%
summarise_at(.funs = funs(mape = mean(.*100)),
.vars = vars(pred1_abs.percent.error:pred2_abs.percent.error))
# errors from the top customer portfolios
error.df2 <-
error.df %>%
left_join(test.df, "customer") %>%
mutate(customer_portfolio = cut(x = rev <- revenue,
breaks = qu <- quantile(rev, probs = seq(0, 1, 0.25)),
labels = names(qu)[-1],
include.lowest = T))
portfolio.mae <-
error.df2 %>%
group_by(customer_portfolio) %>%
summarise_at(.funs = funs(mae = mean(.)),
.vars = vars(pred1_abs.error:pred2_abs.error)) %>%
ungroup()
portfolio.mae <-
error.df2 %>%
group_by(customer_portfolio) %>%
summarise_at(.funs = funs(mae = mean(.*100)),
.vars = vars(pred1_abs.error:pred2_abs.error)) %>%
ungroup()
portfolio.errors <-
portfolio.mae %>%
left_join(portfolio.mape, "customer_portfolio") %>%
gather(key = error_type, value = error, -customer_portfolio) %>%
mutate(error_type2 = ifelse(grepl(pattern = "mae", error_type),"MAE","MAPE"),
model_type = ifelse(grepl(pattern = "pred1", error_type),"Untuned Forest",
ifelse(grepl(pattern = "pred2", error_type),"Tuned Forest",
ifelse(grepl(pattern = "pred3", error_type),"Linear Model",
ifelse(grepl(pattern = "pred4", error_type),"Non-linear Model A",ifelse(grepl(pattern = "pred5", error_type),"Non-linear Model B","Non-linear w interaction"))))),
model_type_reordered = factor(model_type, levels = c("Untuned Forest","Tuned Forest")))
?partial
?plotPartial
??plotPartial
?partial
i=acq_vars[1]
j=acq_vars[5]
means.exp<-partial(untuned_acq,partial.xvar = i,partial.xvar2=j,
partial.values = sequence_,partial.values2=sequence_)$regrOutput$acquisition %>% colMeans()
i=acq_vars[1]
j=acq_vars[5]
sequence1<-seq(quantile(AR[[i]])[[2]],quantile(AR[[i]])[[4]],(quantile(AR[[i]])[[2]]-quantile(AR[[i]])[[4]])/-30)
sequence2<-seq(quantile(AR[[j]])[[2]],quantile(AR[[j]])[[4]],(quantile(AR[[j]])[[2]]-quantile(AR[[j]])[[4]])/-30)
means.exp<-partial(untuned_acq,partial.xvar = i,partial.xvar2=j,
partial.values = sequence1,partial.values2=sequence2)
length(sequence1)
length(sequence2)
i=acq_vars[1]
j=acq_vars[5]
sequence1<-seq(quantile(AR[[i]])[[2]],quantile(AR[[i]])[[4]],(quantile(AR[[i]])[[2]]-quantile(AR[[i]])[[4]])/-30)
sequence2<-seq(quantile(AR[[j]])[[2]],quantile(AR[[j]])[[4]],(quantile(AR[[j]])[[2]]-quantile(AR[[j]])[[4]])/-30)
means.exp<-partial(untuned_acq,partial.xvar = c(i,j),partial.values = c(sequence1,sequence2))
knitr::opts_chunk$set(echo = TRUE)
library(SMCRM) # CRM data
library(dplyr) # data wrangling
library(tidyr) # data wrangling
library(ggplot2) # plotting
library(survival) # survival
library(rpart) # DT
library(randomForestSRC) # RF
library(sampleSelection)
library(pdp)
install.packages("pdp")
library(pdp)
par.1 <- partial(untuned_acq, pred.var = c(i), chull = TRUE)
plot.1<- autoplot(par.1, contour = TRUE)
# Two Variables
par.3 <- partial(untuned_acq, pred.var = c(i,j), chull = TRUE)
j=acq_vars[4]
par.1 <- partial(untuned_acq, pred.var = c(i), chull = TRUE)
acq_vars
par.1 <- partial(untuned_acq, pred.var = c(i), chull = TRUE)
par.1 <- pdp::partial(untuned_acq, pred.var = c(i), chull = TRUE)
plot.1<- autoplot(par.1, contour = TRUE)
# Two Variables
par.3 <- pdp::partial(untuned_acq, pred.var = c("acq_exp","employees"), chull = TRUE)
?pdp::partial
par.1 <- pdp::partial(untuned_acq, pred.var = c(i), type="classification",chull = TRUE)
par.1 <- pdp::partial(untuned_acq, pred.var = c(i), type="auto",chull = TRUE)
partial.rfsrc(untuned_acq,xvar.names=c("acq_exp","revenue"))
i<-dur_vars[2]
j<-dur_vars[3]
par.1 <- pdp::partial(untuned_dur, pred.var = c(i),chull = TRUE)
plot.1<- autoplot(par.1, contour = TRUE)
# Two Variables
par.3 <- pdp::partial(untuned_dur, pred.var = c(i,j),chull = TRUE)
plot.3 <- autoplot(par.3, contour = TRUE,
legend.title = "Partial\Independence")
?find_interaction
?find.interaction
knitr::opts_chunk$set(echo = TRUE)
library(SMCRM) # CRM data
library(dplyr) # data wrangling
library(tidyr) # data wrangling
library(ggplot2) # plotting
library(rpart) # DT
library(randomForestSRC) # RF
library(sampleSelection)
data(acquisitionRetention)
AR<-acquisitionRetention
idx.train <- sample(1:nrow(AR), size = 0.7 * nrow(AR))
train <- AR[idx.train,]
test <- AR[-idx.train,]
set.seed(123)
find.interaction(untuned_acq,
method = "vimp",
importance = "permute")
find.interaction(untuned_dur,
method = "vimp",
importance = "permute")
