---
title: "CustomerAcquisitionRetention2"
author: "Visha Arumugam, Michael Grogan,Sanyogita Apte"
date: "11/12/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)


library(SMCRM) # CRM data
library(dplyr) # data wrangling
library(tidyr) # data wrangling
library(ggplot2) # plotting
library(survival) # survival
library(rpart) # DT
library(randomForestSRC) # RF
library(sampleSelection)
library(micEcon)


data(acquisitionRetention)
AR<-acquisitionRetention

idx.train <- sample(1:nrow(AR), size = 0.7 * nrow(AR))
train <- AR[idx.train,]
test <- AR[-idx.train,]

set.seed(123)



```



Formulas
```{r}
acq_vars<-c("acq_exp","acq_exp_sq","industry","revenue","employees")
acq_target<-"acquisition"

acq_formula<-as.formula(paste(acq_target,paste(acq_vars,collapse="+"),sep="~"))


dur_vars<-c("ret_exp","ret_exp_sq","freq","freq_sq","crossbuy","sow","IMR")

dur_target<-"duration"

dur_formula<-as.formula(paste(dur_target,paste(dur_vars,collapse="+"),sep="~"))

```




Logistic Regression
```{r}
#Logistic Acquisition prediction
acq_mod<-glm(acq_formula,train,family=binomial(link="probit"))
train$IMR<-invMillsRatio(acq_mod)$IMR1


trainbaseline<-ifelse(predict(acq_mod,train,type="response")>0.5,1,0)
table(trainbaseline,train$acquisition)
print(sum(trainbaseline==train$acquisition)/length(train$acquisition))
```


```{r}
acq_pred<-ifelse(predict(acq_mod,test,type="response")>0.5,1,0)

table(acq_pred,test$acquisition)
print(sum(acq_pred==test$acquisition)/length(test$acquisition))
```

```{r}
#Logistic Duration Prediction
#dur_mod<-glm(dur_formula,train[train$acquisition==1,],family=gaussian)
dur_mod<-glm(dur_formula,train,family=gaussian)

sqrt(mean(dur_mod$residuals^2))
```
```{r}

test$IMR<-invMillsRatio(glm(acq_formula,test,family=binomial(link="probit")))$IMR1

dur_pred<-predict(dur_mod,test)
sqrt(mean((dur_pred-test$duration)^2))
```

Single Decision Tree


```{r}
dt_acq<-rpart(acq_formula,train)
dt_dur<-rpart(dur_formula,train)




```



```{r}
trainbaselinedt<-ifelse(predict(dt_acq,train)>0.5,1,0)
table(trainbaselinedt,train$acquisition)
print(sum(trainbaseline==train$acquisition)/length(train$acquisition))


```



```{r}
acq_pred<-ifelse(predict(dt_acq,test)>0.5,1,0)

table(acq_pred,test$acquisition)
print(sum(acq_pred==test$acquisition)/length(test$acquisition))

```

```{r}

dur_pred<-predict(dt_dur,train)
sqrt(mean((dur_pred-train$duration)^2))
```

```{r}

dur_pred<-predict(dt_dur,test)
sqrt(mean((dur_pred-test$duration)^2))
```










Random Forest

```{r}
# Establish a list of possible values for hyper-parameters
mtry.values <- seq(4,6,1)
nodesize.values <- seq(4,8,2)
ntree.values <- seq(4e3,6e3,1e3)

# Create a data frame containing all combinations 
hyper_grid <- expand.grid(mtry = mtry.values, nodesize = nodesize.values, ntree = ntree.values)

# Create an empty vector to store OOB error values
oob_err <- c()

# Write a loop over the rows of hyper_grid to train the grid of models
for (i in 1:nrow(hyper_grid)) {

    # Train a Random Forest model
   model <- rfsrc(acq_formula,data = train,
                            mtry = hyper_grid$mtry[i],
                            nodesize = hyper_grid$nodesize[i],
                            ntree = hyper_grid$ntree[i])  
  
                          
    # Store OOB error for the model                      
    oob_err[i] <- model$err.rate[length(model$err.rate)]
}

# Identify optimal set of hyperparmeters based on OOB error
opt_i <- which.min(oob_err)



tuned_acq <- rfsrc(acq_formula,data = train,
                            mtry = hyper_grid[opt_i,1],
                            nodesize = hyper_grid[opt_i,2],
                            ntree = hyper_grid[opt_i,3])
```


```{r}
trainbaselinerf<-ifelse(predict(tuned_acq,train)$predicted>0.5,1,0)
table(trainbaselinerf,train$acquisition)
print(sum(trainbaseline==train$acquisition)/length(train$acquisition))


```



```{r}
acq_predrf<-ifelse(predict(tuned_acq,test)$predicted>0.5,1,0)

table(acq_pred,test$acquisition)
print(sum(acq_pred==test$acquisition)/length(test$acquisition))

```




```{r}
# Establish a list of possible values for hyper-parameters
mtry.values <- seq(4,6,1)
nodesize.values <- seq(4,8,2)
ntree.values <- seq(4e3,6e3,1e3)

# Create a data frame containing all combinations 
hyper_grid <- expand.grid(mtry = mtry.values, nodesize = nodesize.values, ntree = ntree.values)

# Create an empty vector to store OOB error values
oob_err <- c()

# Write a loop over the rows of hyper_grid to train the grid of models
for (i in 1:nrow(hyper_grid)) {

    # Train a Random Forest model
   model <- rfsrc(dur_formula,data = train,
                            mtry = hyper_grid$mtry[i],
                            nodesize = hyper_grid$nodesize[i],
                            ntree = hyper_grid$ntree[i])  
  
                          
    # Store OOB error for the model                      
    oob_err[i] <- model$err.rate[length(model$err.rate)]
}

# Identify optimal set of hyperparmeters based on OOB error
opt_i <- which.min(oob_err)



tuned_dur <- rfsrc(dur_formula,data = train,
                            mtry = hyper_grid[opt_i,1],
                            nodesize = hyper_grid[opt_i,2],
                            ntree = hyper_grid[opt_i,3]) 



dur_predrf<-predict(tuned_dur,train)$predicted
sqrt(mean((dur_predrf-train$duration)^2))
     
```

```{r}

dur_predrf<-predict(tuned_dur,test)$predicted
sqrt(mean((dur_predrf-test$duration)^2))


```


```{r}

```

